{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456780)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prevents a nasty bug: using preprocess_input within a Dataset\n",
    "# throws an error due to a global variable not being initialized \n",
    "# in the tensor context\n",
    "_ = vgg19.preprocess_input(tf.zeros((1, 224, 224, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-sweet",
   "metadata": {},
   "source": [
    "# Sneaker Type Classification\n",
    "## Classifying series of Jordan basketball sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-banana",
   "metadata": {},
   "source": [
    "Dataset location: https://www.kaggle.com/sebastiaanjohn/sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"jordans\"\n",
    "DATA_DIR = \"data_descriptors\"\n",
    "\n",
    "TRAIN_PCT = 0.8\n",
    "VAL_PCT = 0.1\n",
    "TEST_PCT = 0.1\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "IMAGE_SIZE_INPUT = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filenames = {}\n",
    "for model_dir in os.listdir(BASE_DIR):\n",
    "    model_dir_parts = model_dir.split(\" \", maxsplit = 2)\n",
    "    model_name = f\"{model_dir_parts[0]} {model_dir_parts[1]}\"\n",
    "    print(model_dir)\n",
    "    \n",
    "    full_model_dir = os.path.join(BASE_DIR, model_dir)\n",
    "    files_in_model_dir = os.listdir(full_model_dir)\n",
    "    files_in_model_dir = [os.path.join(full_model_dir, file) for file in files_in_model_dir]\n",
    "    files_in_model_dir = [os.path.abspath(file) for file in files_in_model_dir]\n",
    "    \n",
    "    if model_name not in model_filenames:\n",
    "        model_filenames[model_name] = []\n",
    "    \n",
    "    model_filenames[model_name].extend(files_in_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filenames_df = pd.DataFrame({\"filename\": [], \"model\": []})\n",
    "\n",
    "for (model_name, filenames) in model_filenames.items():\n",
    "    records = [{\"filename\": filename, \"model\": model_name} for filename in filenames]\n",
    "    model_filenames_df = model_filenames_df.append(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_by_num_images = model_filenames_df.groupby(\"model\").size()\n",
    "plt.barh(groups_by_num_images.index, groups_by_num_images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_by_num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_to_test = model_filenames_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-mystery",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_images(image_descriptors):\n",
    "    for filename, model_class in image_descriptors.values:\n",
    "        try:\n",
    "            image = imread(filename)\n",
    "            plt.imshow(image)\n",
    "            plt.title(model_class)\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(f\"Could not read {filename}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(filenames_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_select = [\"Jordan 1\", \"Jordan 4\", \"Jordan 6\", \"Jordan 11\"]\n",
    "models_map = {\n",
    "    \"Jordan 1\": 0,\n",
    "    \"Jordan 4\": 1,\n",
    "    \"Jordan 6\": 2,\n",
    "    \"Jordan 11\": 3,\n",
    "}\n",
    "\n",
    "max_num_samples_to_select = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_filenames_df = model_filenames_df[model_filenames_df.model.isin(models_to_select)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readable_files(filenames):\n",
    "    readable_files = []\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            imread(filename)\n",
    "            readable_files.append(True)\n",
    "        except:\n",
    "            readable_files.append(False)\n",
    "            continue\n",
    "    return readable_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "readable_files = get_readable_files(selected_model_filenames_df.filename.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_filenames_df[\"is_readable\"] = readable_files\n",
    "selected_model_filenames_df.is_readable = pd.Series(selected_model_filenames_df.is_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_filenames_df = selected_model_filenames_df[selected_model_filenames_df.is_readable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_filenames_balanced = pd.DataFrame()\n",
    "for (model_name, group_filenames) in selected_model_filenames_df.groupby(\"model\"):\n",
    "    readable_filenames = group_filenames[group_filenames.is_readable]\n",
    "    selected_model_filenames_balanced = selected_model_filenames_balanced.append(\n",
    "        readable_filenames.sample(max_num_samples_to_select, replace = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_filenames_balanced = selected_model_filenames_balanced.drop(\"is_readable\", axis = 1)\n",
    "selected_model_filenames_balanced.to_csv(os.path.join(DATA_DIR, \"filenames.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    train_data = pd.DataFrame()\n",
    "    val_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    for (model_name, group_filenames) in dataset.groupby(\"model\"):\n",
    "        group_filenames = group_filenames.sample(len(group_filenames))\n",
    "\n",
    "        train_data_end_index = int(len(group_filenames) * TRAIN_PCT)\n",
    "        val_data_end_index = train_data_end_index + int(len(group_filenames) * VAL_PCT)\n",
    "\n",
    "        train_data_in_group = group_filenames[:train_data_end_index]\n",
    "        val_data_in_group = group_filenames[train_data_end_index:val_data_end_index]\n",
    "        test_data_in_group = group_filenames[val_data_end_index:]\n",
    "\n",
    "        train_data = train_data.append(train_data_in_group)\n",
    "        val_data = val_data.append(val_data_in_group)    \n",
    "        test_data = test_data.append(test_data_in_group)\n",
    "        \n",
    "    return (train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(selected_model_filenames_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(DATA_DIR, \"train.csv\"), index = False)\n",
    "val_data.to_csv(os.path.join(DATA_DIR, \"val.csv\"), index = False)\n",
    "test_data.to_csv(os.path.join(DATA_DIR, \"test.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(\"model\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = train_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample.model = train_data_sample.model.map(models_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(train_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename, model):\n",
    "    file = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(file)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = image / 256.0\n",
    "    return (image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "poc_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_data_sample.filename.values, train_data_sample.model.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "poc_dataset = poc_dataset.map(read_image)\n",
    "poc_dataset = poc_dataset.batch(10)\n",
    "poc_dataset = poc_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential([\n",
    "    Input(IMAGE_SIZE_INPUT),\n",
    "    Conv2D(64, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(32, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(16, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n",
    "    Flatten(),\n",
    "    Dense(16, activation = \"relu\"),\n",
    "    Dropout(0.25),\n",
    "    Dense(len(models_to_select), activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(poc_dataset, epochs = 30, steps_per_epoch = 1, callbacks = [TensorBoard()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.evaluate(poc_dataset, steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = vgg19.VGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_vgg(filename, model):\n",
    "    file = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(file)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = vgg19.preprocess_input(image)\n",
    "    return (image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-planner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_data_sample.filename.values, train_data_sample.model.values))\n",
    "vgg_dataset = vgg_dataset.map(read_image_vgg)\n",
    "vgg_dataset = vgg_dataset.batch(10)\n",
    "vgg_dataset = vgg_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(vgg_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_transfer = Model(inputs = vgg_model.layers[0].input, outputs = vgg_model.layers[22].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg_transfer.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_transfer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = Sequential([\n",
    "    vgg_transfer,\n",
    "    Dense(8, activation = \"relu\"),\n",
    "    Dense(len(models_to_select), activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-linux",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfer_model.fit(vgg_dataset, epochs = 30, steps_per_epoch = 1, callbacks = [TensorBoard(log_dir = \"logs_transfer_original\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = Sequential([\n",
    "    vgg_transfer,\n",
    "    Dense(16, activation = \"relu\"),\n",
    "    Dense(len(models_to_select), activation = \"softmax\")\n",
    "])\n",
    "\n",
    "transfer_model.compile(optimizer = RMSprop(learning_rate = 0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fit(vgg_dataset, epochs = 30, steps_per_epoch = 1, callbacks = [TensorBoard(log_dir = \"logs_transfer_larger\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
